import asyncio
import json as json_lib
import logging
import gzip
import os
import tempfile
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, asdict
import aiofiles
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, text, inspect
from sqlalchemy.orm import selectinload

from app.config import settings
from app.database.database import get_db, engine
from app.database.models import (
    User, Subscription, Transaction, PromoCode, PromoCodeUse,
    ReferralEarning, Squad, ServiceRule, SystemSetting, MonitoringLog,
    SubscriptionConversion, SentNotification, BroadcastHistory,
    ServerSquad, SubscriptionServer, UserMessage, YooKassaPayment,
    CryptoBotPayment, Base
)

logger = logging.getLogger(__name__)


@dataclass
class BackupMetadata:
    timestamp: str
    version: str = "1.0"
    database_type: str = "postgresql"
    backup_type: str = "full"
    tables_count: int = 0
    total_records: int = 0
    compressed: bool = True
    file_size_bytes: int = 0
    created_by: Optional[int] = None


@dataclass
class BackupSettings:
    auto_backup_enabled: bool = True
    backup_interval_hours: int = 24
    backup_time: str = "03:00"
    max_backups_keep: int = 7
    compression_enabled: bool = True
    include_logs: bool = False
    backup_location: str = "/app/data/backups"


class BackupService:
    
    def __init__(self, bot=None):
        self.bot = bot
        self.backup_dir = Path(settings.SQLITE_PATH).parent / "backups"
        self.backup_dir.mkdir(exist_ok=True)
        self._auto_backup_task = None
        self._settings = self._load_settings()
        
        self.backup_models = [
            User, Subscription, Transaction, PromoCode, PromoCodeUse,
            ReferralEarning, ServiceRule, SystemSetting,
            SubscriptionConversion, SentNotification, BroadcastHistory,
            ServerSquad, SubscriptionServer, UserMessage,
            YooKassaPayment, CryptoBotPayment
        ]
        
        if self._settings.include_logs:
            self.backup_models.append(MonitoringLog)

    def _load_settings(self) -> BackupSettings:
        return BackupSettings(
            auto_backup_enabled=os.getenv("BACKUP_AUTO_ENABLED", "true").lower() == "true",
            backup_interval_hours=int(os.getenv("BACKUP_INTERVAL_HOURS", "24")),
            backup_time=os.getenv("BACKUP_TIME", "03:00"),
            max_backups_keep=int(os.getenv("BACKUP_MAX_KEEP", "7")),
            compression_enabled=os.getenv("BACKUP_COMPRESSION", "true").lower() == "true",
            include_logs=os.getenv("BACKUP_INCLUDE_LOGS", "false").lower() == "true",
            backup_location=os.getenv("BACKUP_LOCATION", "/app/data/backups")
        )

    async def create_backup(
        self, 
        created_by: Optional[int] = None,
        compress: bool = True,
        include_logs: bool = None
    ) -> Tuple[bool, str, Optional[str]]:
        try:
            logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –±–µ–∫–∞–ø–∞...")
            
            if include_logs is None:
                include_logs = self._settings.include_logs
            
            models_to_backup = self.backup_models.copy()
            if not include_logs and MonitoringLog in models_to_backup:
                models_to_backup.remove(MonitoringLog)
            elif include_logs and MonitoringLog not in models_to_backup:
                models_to_backup.append(MonitoringLog)
            
            backup_data = {}
            total_records = 0
            
            async for db in get_db():
                try:
                    for model in models_to_backup:
                        table_name = model.__tablename__
                        logger.info(f"üìä –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É: {table_name}")
                        
                        result = await db.execute(select(model))
                        records = result.scalars().all()
                        
                        table_data = []
                        for record in records:
                            record_dict = {}
                            for column in model.__table__.columns:
                                value = getattr(record, column.name)
                                
                                if isinstance(value, datetime):
                                    record_dict[column.name] = value.isoformat()
                                elif hasattr(value, '__dict__'):
                                    record_dict[column.name] = str(value)
                                else:
                                    record_dict[column.name] = value
                            
                            table_data.append(record_dict)
                        
                        backup_data[table_name] = table_data
                        total_records += len(table_data)
                        
                        logger.info(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(table_data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {table_name}")
                    
                    break
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
                    raise e
                finally:
                    await db.close()
            
            metadata = BackupMetadata(
                timestamp=datetime.utcnow().isoformat(),
                database_type="postgresql" if settings.is_postgresql() else "sqlite",
                backup_type="full",
                tables_count=len(models_to_backup),
                total_records=total_records,
                compressed=compress,
                created_by=created_by,
                file_size_bytes=0
            )
            
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            filename = f"backup_{timestamp}.json"
            if compress:
                filename += ".gz"
            
            backup_path = self.backup_dir / filename
            
            backup_structure = {
                "metadata": asdict(metadata),
                "data": backup_data
            }
            
            if compress:
                backup_json_str = json_lib.dumps(backup_structure, ensure_ascii=False, indent=2)
                async with aiofiles.open(backup_path, 'wb') as f:
                    compressed_data = gzip.compress(backup_json_str.encode('utf-8'))
                    await f.write(compressed_data)
            else:
                async with aiofiles.open(backup_path, 'w', encoding='utf-8') as f:
                    await f.write(json_lib.dumps(backup_structure, ensure_ascii=False, indent=2))
            
            file_size = backup_path.stat().st_size
            backup_structure["metadata"]["file_size_bytes"] = file_size
            
            if compress:
                backup_json_str = json_lib.dumps(backup_structure, ensure_ascii=False, indent=2)
                async with aiofiles.open(backup_path, 'wb') as f:
                    compressed_data = gzip.compress(backup_json_str.encode('utf-8'))
                    await f.write(compressed_data)
            else:
                async with aiofiles.open(backup_path, 'w', encoding='utf-8') as f:
                    await f.write(json_lib.dumps(backup_structure, ensure_ascii=False, indent=2))
            
            await self._cleanup_old_backups()
            
            size_mb = file_size / 1024 / 1024
            message = (f"‚úÖ –ë–µ–∫–∞–ø —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!\n"
                      f"üìÅ –§–∞–π–ª: {filename}\n"
                      f"üìä –¢–∞–±–ª–∏—Ü: {len(models_to_backup)}\n"
                      f"üìà –ó–∞–ø–∏—Å–µ–π: {total_records:,}\n"
                      f"üíæ –†–∞–∑–º–µ—Ä: {size_mb:.2f} MB")
            
            logger.info(message)
            
            if self.bot:
                await self._send_backup_notification(
                    "success", message, str(backup_path)
                )
            
            return True, message, str(backup_path)
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±–µ–∫–∞–ø–∞: {str(e)}"
            logger.error(error_msg, exc_info=True)
            
            if self.bot:
                await self._send_backup_notification("error", error_msg)
            
            return False, error_msg, None

    async def restore_backup(
        self, 
        backup_file_path: str,
        clear_existing: bool = False
    ) -> Tuple[bool, str]:
        try:
            logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ {backup_file_path}")
            
            backup_path = Path(backup_file_path)
            if not backup_path.exists():
                return False, f"‚ùå –§–∞–π–ª –±–µ–∫–∞–ø–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {backup_file_path}"
            
            if backup_path.suffix == '.gz':
                async with aiofiles.open(backup_path, 'rb') as f:
                    compressed_data = await f.read()
                    uncompressed_data = gzip.decompress(compressed_data).decode('utf-8')
                    backup_structure = json_lib.loads(uncompressed_data)
            else:
                async with aiofiles.open(backup_path, 'r', encoding='utf-8') as f:
                    file_content = await f.read()
                    backup_structure = json_lib.loads(file_content)
            
            metadata = backup_structure.get("metadata", {})
            backup_data = backup_structure.get("data", {})
            
            if not backup_data:
                return False, "‚ùå –§–∞–π–ª –±–µ–∫–∞–ø–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö"
            
            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω –±–µ–∫–∞–ø –æ—Ç {metadata.get('timestamp')}")
            logger.info(f"üìà –°–æ–¥–µ—Ä–∂–∏—Ç {metadata.get('total_records', 0)} –∑–∞–ø–∏—Å–µ–π")
            
            restored_records = 0
            restored_tables = 0
            
            async for db in get_db():
                try:
                    if clear_existing:
                        logger.warning("üóëÔ∏è –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ...")
                        await self._clear_database_tables(db)
                    
                    for table_name, records in backup_data.items():
                        if not records:
                            continue
                        
                        model = None
                        for m in self.backup_models:
                            if m.__tablename__ == table_name:
                                model = m
                                break
                        
                        if not model:
                            logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {table_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                            continue
                        
                        logger.info(f"üì• –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É {table_name} ({len(records)} –∑–∞–ø–∏—Å–µ–π)")
                        
                        for record_data in records:
                            try:
                                processed_data = {}
                                for key, value in record_data.items():
                                    if value is None:
                                        processed_data[key] = None
                                        continue
                                    
                                    column = getattr(model.__table__.columns, key, None)
                                    if column is None:
                                        continue
                                    
                                    column_type_str = str(column.type).upper()
                                    if ('DATETIME' in column_type_str or 'TIMESTAMP' in column_type_str) and isinstance(value, str):
                                        try:
                                            if 'T' in value:
                                                processed_data[key] = datetime.fromisoformat(value.replace('Z', '+00:00'))
                                            else:
                                                processed_data[key] = datetime.strptime(value, '%Y-%m-%d %H:%M:%S')
                                        except (ValueError, TypeError) as e:
                                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É {value} –¥–ª—è –ø–æ–ª—è {key}: {e}")
                                            processed_data[key] = datetime.utcnow()
                                    elif ('BOOLEAN' in column_type_str or 'BOOL' in column_type_str) and isinstance(value, str):
                                        processed_data[key] = value.lower() in ('true', '1', 'yes', 'on')
                                    elif ('INTEGER' in column_type_str or 'INT' in column_type_str) and isinstance(value, str):
                                        try:
                                            processed_data[key] = int(value)
                                        except ValueError:
                                            processed_data[key] = 0
                                    elif ('FLOAT' in column_type_str or 'REAL' in column_type_str or 'NUMERIC' in column_type_str) and isinstance(value, str):
                                        try:
                                            processed_data[key] = float(value)
                                        except ValueError:
                                            processed_data[key] = 0.0
                                    elif 'JSON' in column_type_str and isinstance(value, str):
                                        try:
                                            processed_data[key] = json_lib.loads(value)
                                        except (ValueError, TypeError):
                                            processed_data[key] = value
                                    else:
                                        processed_data[key] = value
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∑–∞–ø–∏—Å—å —Å —Ç–∞–∫–∏–º ID
                                primary_key_col = None
                                for col in model.__table__.columns:
                                    if col.primary_key:
                                        primary_key_col = col.name
                                        break
                                
                                if primary_key_col and primary_key_col in processed_data:
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏
                                    existing_record = await db.execute(
                                        select(model).where(
                                            getattr(model, primary_key_col) == processed_data[primary_key_col]
                                        )
                                    )
                                    existing = existing_record.scalar_one_or_none()
                                    
                                    if existing:
                                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å
                                        for key, value in processed_data.items():
                                            if key != primary_key_col:  # –ù–µ –æ–±–Ω–æ–≤–ª—è–µ–º primary key
                                                setattr(existing, key, value)
                                        logger.debug(f"–û–±–Ω–æ–≤–ª–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∑–∞–ø–∏—Å—å {primary_key_col}={processed_data[primary_key_col]} –≤ {table_name}")
                                    else:
                                        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                                        instance = model(**processed_data)
                                        db.add(instance)
                                else:
                                    # –ï—Å–ª–∏ –Ω–µ—Ç primary key –∏–ª–∏ –æ–Ω –Ω–µ –≤ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º
                                    instance = model(**processed_data)
                                    db.add(instance)
                                
                                restored_records += 1
                                
                            except Exception as e:
                                logger.error(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏ –≤ {table_name}: {e}")
                                continue
                        
                        restored_tables += 1
                        logger.info(f"‚úÖ –¢–∞–±–ª–∏—Ü–∞ {table_name} –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                    
                    await db.commit()
                    
                    break
                    
                except Exception as e:
                    await db.rollback()
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏: {e}")
                    raise e
                finally:
                    await db.close()
            
            message = (f"‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n"
                      f"üìä –¢–∞–±–ª–∏—Ü: {restored_tables}\n"
                      f"üìà –ó–∞–ø–∏—Å–µ–π: {restored_records:,}\n"
                      f"üìÖ –î–∞—Ç–∞ –±–µ–∫–∞–ø–∞: {metadata.get('timestamp', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            
            logger.info(message)
            
            if self.bot:
                await self._send_backup_notification("restore_success", message)
            
            return True, message
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {str(e)}"
            logger.error(error_msg, exc_info=True)
            
            if self.bot:
                await self._send_backup_notification("restore_error", error_msg)
            
            return False, error_msg

    async def _clear_database_tables(self, db: AsyncSession):
        tables_order = [
            "subscription_servers", "sent_notifications", "broadcast_history",
            "subscription_conversions", "referral_earnings", "promocode_uses",
            "transactions", "yookassa_payments", "cryptobot_payments",
            "subscriptions", "users", "promocodes", "server_squads",
            "service_rules", "system_settings", "monitoring_logs", "user_messages"
        ]
        
        for table_name in tables_order:
            try:
                await db.execute(text(f"DELETE FROM {table_name}"))
                logger.info(f"üóëÔ∏è –û—á–∏—â–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ {table_name}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É {table_name}: {e}")

    async def get_backup_list(self) -> List[Dict[str, Any]]:
        backups = []
        
        try:
            for backup_file in sorted(self.backup_dir.glob("backup_*.json*"), reverse=True):
                try:
                    if backup_file.suffix == '.gz':
                        with gzip.open(backup_file, 'rt', encoding='utf-8') as f:
                            backup_structure = json_lib.load(f)
                    else:
                        with open(backup_file, 'r', encoding='utf-8') as f:
                            backup_structure = json_lib.load(f)
                    
                    metadata = backup_structure.get("metadata", {})
                    file_stats = backup_file.stat()
                    
                    backup_info = {
                        "filename": backup_file.name,
                        "filepath": str(backup_file),
                        "timestamp": metadata.get("timestamp"),
                        "tables_count": metadata.get("tables_count", 0),
                        "total_records": metadata.get("total_records", 0),
                        "compressed": metadata.get("compressed", False),
                        "file_size_bytes": file_stats.st_size,
                        "file_size_mb": round(file_stats.st_size / 1024 / 1024, 2),
                        "created_by": metadata.get("created_by"),
                        "database_type": metadata.get("database_type", "unknown")
                    }
                    
                    backups.append(backup_info)
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö {backup_file}: {e}")
                    file_stats = backup_file.stat()
                    backups.append({
                        "filename": backup_file.name,
                        "filepath": str(backup_file),
                        "timestamp": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                        "tables_count": "?",
                        "total_records": "?",
                        "compressed": backup_file.suffix == '.gz',
                        "file_size_bytes": file_stats.st_size,
                        "file_size_mb": round(file_stats.st_size / 1024 / 1024, 2),
                        "created_by": None,
                        "database_type": "unknown",
                        "error": f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {str(e)}"
                    })
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –±–µ–∫–∞–ø–æ–≤: {e}")
        
        return backups

    async def delete_backup(self, backup_filename: str) -> Tuple[bool, str]:
        try:
            backup_path = self.backup_dir / backup_filename
            
            if not backup_path.exists():
                return False, f"‚ùå –§–∞–π–ª –±–µ–∫–∞–ø–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {backup_filename}"
            
            backup_path.unlink()
            message = f"‚úÖ –ë–µ–∫–∞–ø {backup_filename} —É–¥–∞–ª–µ–Ω"
            logger.info(message)
            
            return True, message
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –±–µ–∫–∞–ø–∞: {str(e)}"
            logger.error(error_msg)
            return False, error_msg

    async def _cleanup_old_backups(self):
        try:
            backups = await self.get_backup_list()
            
            if len(backups) > self._settings.max_backups_keep:
                backups.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
                
                for backup in backups[self._settings.max_backups_keep:]:
                    try:
                        await self.delete_backup(backup["filename"])
                        logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –±–µ–∫–∞–ø: {backup['filename']}")
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–∞—Ä–æ–≥–æ –±–µ–∫–∞–ø–∞ {backup['filename']}: {e}")
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –±–µ–∫–∞–ø–æ–≤: {e}")

    async def get_backup_settings(self) -> BackupSettings:
        return self._settings

    async def update_backup_settings(self, **kwargs) -> bool:
        try:
            for key, value in kwargs.items():
                if hasattr(self._settings, key):
                    setattr(self._settings, key, value)
            
            if self._settings.auto_backup_enabled:
                await self.start_auto_backup()
            else:
                await self.stop_auto_backup()
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –±–µ–∫–∞–ø–æ–≤: {e}")
            return False

    async def start_auto_backup(self):
        if self._auto_backup_task and not self._auto_backup_task.done():
            self._auto_backup_task.cancel()
        
        if self._settings.auto_backup_enabled:
            self._auto_backup_task = asyncio.create_task(self._auto_backup_loop())
            logger.info(f"üîÑ –ê–≤—Ç–æ–±–µ–∫–∞–ø—ã –≤–∫–ª—é—á–µ–Ω—ã, –∏–Ω—Ç–µ—Ä–≤–∞–ª: {self._settings.backup_interval_hours}—á")

    async def stop_auto_backup(self):
        if self._auto_backup_task and not self._auto_backup_task.done():
            self._auto_backup_task.cancel()
            logger.info("‚èπÔ∏è –ê–≤—Ç–æ–±–µ–∫–∞–ø—ã –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")

    async def _auto_backup_loop(self):
        while True:
            try:
                await asyncio.sleep(self._settings.backup_interval_hours * 3600)
                
                logger.info("üîÑ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –±–µ–∫–∞–ø–∞...")
                success, message, _ = await self.create_backup()
                
                if success:
                    logger.info(f"‚úÖ –ê–≤—Ç–æ–±–µ–∫–∞–ø –∑–∞–≤–µ—Ä—à–µ–Ω: {message}")
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–±–µ–∫–∞–ø–∞: {message}")
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –∞–≤—Ç–æ–±–µ–∫–∞–ø–æ–≤: {e}")
                await asyncio.sleep(3600)

    async def _send_backup_notification(
        self, 
        event_type: str, 
        message: str, 
        file_path: str = None
    ):
        try:
            if not settings.is_admin_notifications_enabled():
                return
            
            icons = {
                "success": "‚úÖ",
                "error": "‚ùå", 
                "restore_success": "üì•",
                "restore_error": "‚ùå"
            }
            
            icon = icons.get(event_type, "‚ÑπÔ∏è")
            notification_text = f"{icon} <b>–°–ò–°–¢–ï–ú–ê –ë–ï–ö–ê–ü–û–í</b>\n\n{message}"
            
            if file_path:
                notification_text += f"\nüìÅ <code>{Path(file_path).name}</code>"
            
            notification_text += f"\n\n‚è∞ <i>{datetime.now().strftime('%d.%m.%Y %H:%M:%S')}</i>"
            
            try:
                from app.services.admin_notification_service import AdminNotificationService
                admin_service = AdminNotificationService(self.bot)
                await admin_service._send_message(notification_text)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ AdminNotificationService: {e}")
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –±–µ–∫–∞–ø–µ: {e}")


backup_service = BackupService()
